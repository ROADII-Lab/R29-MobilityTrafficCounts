{
    "features": [
        "tmc_code",
        "measurement_tstamp",
        "speed_All",
        "data_density_All",
        "data_density_Pass",
        "data_density_Truck",
        "travel_time_seconds_All",
        "start_latitude",
        "start_longitude",
        "end_latitude",
        "end_longitude",
        "miles",
        "aadt",
        "urban_code",
        "thrulanes_unidir",
        "f_system",
        "route_sign",
        "thrulanes",
        "zip",
        "DIR",
        "TMC_Value",
        "year",
        "month",
        "day",
        "hour",
        "dayofweek",
        "measurement_tstamp_before",
        "measurement_tstamp_after",
        "speed_All_before",
        "speed_All_after",
        "travel_time_seconds_All_before",
        "travel_time_seconds_All_after",
        "data_density_All_before",
        "data_density_All_after",
        "data_density_Pass_before",
        "data_density_Pass_after",
        "data_density_Truck_before",
        "data_density_Truck_after",
        "year_before",
        "year_after",
        "month_before",
        "month_after",
        "day_before",
        "day_after",
        "hour_before",
        "hour_after",
        "dayofweek_before",
        "dayofweek_after"
    ],
    "target": "VOL",
    "training_split": 0.1,
    "model": "LinearNN(\n  (fc1): Linear(in_features=48, out_features=1200, bias=True)\n  (bn1): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (fc2): Linear(in_features=1200, out_features=600, bias=True)\n  (bn2): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (fc3): Linear(in_features=600, out_features=600, bias=True)\n  (bn3): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (fc4): Linear(in_features=600, out_features=300, bias=True)\n  (bn4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (fc5): Linear(in_features=300, out_features=1, bias=True)\n  (dropout): Dropout(p=0.15, inplace=False)\n)",
    "model_version": 1,
    "model_top": "LinearNN(\n  (fc1): Linear(in_features=48, out_features=1200, bias=True)\n  (bn1): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (fc2): Linear(in_features=1200, out_features=600, bias=True)\n  (bn2): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (fc3): Linear(in_features=600, out_features=600, bias=True)\n  (bn3): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (fc4): Linear(in_features=600, out_features=300, bias=True)\n  (bn4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (fc5): Linear(in_features=300, out_features=1, bias=True)\n  (dropout): Dropout(p=0.15, inplace=False)\n)",
    "model_top_loss": 4671.43212890625,
    "model_R2": 0.9890816807746887,
    "model_within_percent": 74.73236872430229,
    "model_filename_root": "../models/model_",
    "model_size": 600,
    "train_loader": "<torch.utils.data.dataloader.DataLoader object at 0x7474ce650410>",
    "test_loader": "<torch.utils.data.dataloader.DataLoader object at 0x7474cf6c0290>",
    "training_epochs": 1500,
    "current_training_epoch": 1500,
    "training_batch_size": 850000,
    "test_batch_size": 850000,
    "training_workers": 16,
    "testing_workers": 4,
    "weight_decay": 0.001,
    "dropout": 0.15,
    "training_learning_rate": 0.03,
    "test_interval": 100,
    "pdiffGoal": 0.15,
    "scaler": "StandardScaler()",
    "device": "cuda",
    "feature_importance_df": "   epoch feature1 feature2 feature3  ...   hour_before    hour_after  dayofweek_before  dayofweek_after\n0    100      NaN      NaN      NaN  ...  72167.562500  38687.437500       8435.042969     10450.875000\n1    200      NaN      NaN      NaN  ...  38049.226562  33099.281250       7655.945312      8450.503906\n2    300      NaN      NaN      NaN  ...  31313.226562  30026.226562       8327.742188      8935.375000\n3    400      NaN      NaN      NaN  ...  24472.000000  26061.542969       7673.042969      8392.023438\n4    500      NaN      NaN      NaN  ...  22414.914062  24968.605469       8272.253906      8613.261719\n5    600      NaN      NaN      NaN  ...  21454.208984  24550.267578      10722.525391     10541.636719\n6    700      NaN      NaN      NaN  ...  18172.349609  20865.978516       9606.847656     10520.556641\n7    800      NaN      NaN      NaN  ...  17456.515625  21867.367188      11788.655273     10865.797852\n8    900      NaN      NaN      NaN  ...  15824.818359  20038.550781      10919.125000      9207.085938\n9   1000      NaN      NaN      NaN  ...  16089.414062  20108.757812      10390.425781     10373.332031\n10  1100      NaN      NaN      NaN  ...  15309.141602  20398.464844       8901.434570      9426.950195\n11  1200      NaN      NaN      NaN  ...  15215.890625  17535.341797      10381.035156      8296.572266\n12  1300      NaN      NaN      NaN  ...  15885.501953  18914.394531       8838.580078      9652.240234\n13  1400      NaN      NaN      NaN  ...  15732.253906  16091.888672       7777.076172      8224.058594\n14  1500      NaN      NaN      NaN  ...  14624.591797  15110.638672       7902.488281      9006.703125\n\n[15 rows x 52 columns]"
}